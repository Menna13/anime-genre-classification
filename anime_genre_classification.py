# -*- coding: utf-8 -*-
"""Anime Genre Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K09TskXUW2iQIqP1JyNmCwKHDi9d0wB7
"""


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
from unidecode import unidecode
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import string

nltk.download('stopwords')
nltk.download('punkt')

import os
import urllib.request
import matplotlib.pyplot as plt
from scipy import spatial
from sklearn.manifold import TSNE
import numpy as np



"""### Loading Data"""
#importing anime dataset from our github repository
anime_df = pd.read_csv('animes.csv')

#Mohamed's genre extraction code
data = anime_df[['title', 'synopsis', 'genre']]
#delete all rows that has null genre
#TODO: get how many null values dropped to include in paper
data = data.dropna()
#print(df['genre'])
genres = set()
for genre in data['genre']:
    #split genre by ',' and igonre first and last characters which are (' and ')
    s = genre[1:-1].split(',')
    for i in s:
        stripped = i.replace('\'', '').replace(' ', '')
        genres.add(stripped)
print(len(genres), genres)
# Total of 43 genre

"""### Preprocess Synopsis"""
data['synopsis'] = data['synopsis'].str.lower()
# pre-processing the synopsis 
anime_corpus = [i for i in data['synopsis']]
def remove_stopwords(data):
  data['synopsis2'] = data['synopsis'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords.words('english'))]))
  return data

def remove_tags(string):
    result = re.sub('<.*?>','',string)
    return result

clean = remove_stopwords(data)
clean['synopsis2']= clean['synopsis2'].apply(lambda cw : remove_tags(cw))
data_without_stopwords['synopsis2'] = data_without_stopwords['synopsis2'].str.replace('[{}]'.format(string.punctuation), ' ')

synopses = clean['synopsis2']
print(synopses)

exit(0);
#Next Step, add GloVe embeddings and if possible try other kinds of embeddings such as tf-idf and word2vec and compare their results with GloVe's

"""### Embeddings"""

def extract_embeddings(path):
  emmbed_dict = {}
  with open(path,'r') as f:
    for word_vector in f:
      v = word_vector.split()
      word = v[0]
      vector = np.asarray(v[1:], 'float32')
      emmbed_dict[word]= vector 
  return emmbed_dict

emmbed_dict = extract_embeddings('/content/glove.6B.200d.txt')
print(emmbed_dict)

#eculidian distance is given to the sorting function to work as a sorting key. eculidian distance function finds the distance between two 1-D arrays
def find_similar_word(word_vector, top_k): 
  nearest = sorted(emmbed_dict.keys(), key = lambda word: spatial.distance.euclidean(emmbed_dict[word],word_vector))
  return nearest[:top_k]

find_similar_word(emmbed_dict['anime'], 10)


